
<script src="https://cdn.rawgit.com/justadudewhohacks/face-api.js/b9ca0b9b/dist/face-api.js"></script>
<script src="common.js"></script>



      

   
<script>
      
document.trainDescriptorsByClass = []        // make it global    
document.detect2 = []
      
      
async function myCheckImage(){
console.log('Perhaps click the ')
//document.getElementById('myDetectButton').click()        
}


async function myLoadImage(myEvent){
    myEvent.preventDefault(); 
    myEvent.stopPropagation();   
    document.getElementById('myImg01').src = await window.URL.createObjectURL(myEvent.dataTransfer.files[0]); 
    setTimeout("document.getElementById('myDetectButton').click()", 200)
}

                                                                  
async function myImageToCanvas(myEvent){
    var c=document.getElementById("myCanvas01");
    var ctx=c.getContext("2d");
    ctx.globalAlpha = 0.5;
    var img=document.getElementById("myImg01");  
    ctx.drawImage(img, 0, 0);
    var img=document.getElementById("overlay");  
    ctx.drawImage(img, 0, 0);
};

</script>



<h2 align=center>Minimal Face-Api.js</h2>
See original github at <a href="https://github.com/justadudewhohacks/face-api.js">https://github.com/justadudewhohacks/face-api.js</a><br>


Save an image of a character from the Big Bang Theory and then drag the image from your computer onto the top of the pre-loaded image.  <br> 




<input type=button value="Load Once" onclick="{
( async function (){
     console.log('Start Loading')
     await faceapi.loadFaceDetectionModel('./weights/')
     await faceapi.loadFaceLandmarkModel('./weights/')    
     await faceapi.loadFaceRecognitionModel('./weights/')
     await console.log('All 3 models loaded, load descriptors')
         
    await document.getElementById('myLoadDescriptors').click()
                                              
     await console.log('Descriptors loaded')
                                              
  })()
}">


<input type=button id="myDetectButton" value=Detect onclick="{
( async function (){

    console.log('Start detect, wait for a bit...')
     // optional arguments
     const minConfidence = 0.8
     const maxResults = 10

     // inputs can be html canvas, img or video element or their ids ...
     myImg = document.getElementById('myImg01')                                // made global not const
     //myImg = document.getElementById('myImgTag')                                // made global not const
     detections = await faceapi.locateFaces(myImg, minConfidence, maxResults)   // made global not const
    console.log('End detect')
                                                             
    await document.getElementById('myDrawButton').click()
  })()
}">







<input type=hidden id="myDrawButton" value=draw onclick="{  // was a button when developing
( async function (){
    console.log('start draw')
    // resize the detected boxes in case your displayed image has a different size then the original
    const detectionsForSize = detections.map(det => det.forSize(myImg.width, myImg.height))
    const canvas = document.getElementById('myCanvas01')
    canvas.width = myImg.width
    canvas.height = myImg.height
    faceapi.drawDetection(canvas, detectionsForSize, { withScore: false }) 
    
    console.log('end draw ')
    console.log(faceapi)
    
    await document.getElementById('myToCanvasButton').click() 
                                                         
                                                         
    })()
}">


<input type=hidden id="myToCanvasButton" value="Image to Canvas" onclick="{   // was a button when developing
    myImageToCanvas()
}">





<input type=button value="Grab Face Areas" onclick="{
( async function (){
console.log('Face Grab Start')
const faceTensors = (await faceapi.extractFaceTensors(myImg, detections))

await console.log('Faces extracted')
                                                    
// detect landmarks and get the aligned face image bounding boxes   // must make it global
alignedFaceBoxes = await Promise.all(faceTensors.map(          
  async (faceTensor, i) => {
   // await  console.log('face #'+i)                                                
    const faceLandmarks = await faceapi.detectLandmarks(faceTensor)
    
   // console.log(detections[i])   
    document.detect2[i] = detections[i]                                                
    return faceLandmarks.align(detections[i])
  }                                                  
))

await console.log('Done Aligned')
// free memory for face image tensors after we detected the face landmarks
faceTensors.forEach(t => t.dispose())

await console.log('Disposes tensors')
// get the face tensors for the aligned face images from the image (have to be disposed manually)
const alignedFaceTensors = (await faceapi.extractFaceTensors(myImg, alignedFaceBoxes))

await console.log('extracted')
// compute the face descriptors from the aligned face images
const descriptors = await Promise.all(alignedFaceTensors.map(
  faceTensor => faceapi.computeFaceDescriptor(faceTensor)
))

await console.log('Computed, now drawing text')
                                                    
         
//console.log('alignedFaceTensors[0].id = '+alignedFaceTensors[0].id)
                                                    
////////////////// from common.js ///////////////////////////////                                                    
   
                                                    
                                               
                                                    
                                                    
                                                    
                                                    
////////////////////////////////////////////////////////////////////////////////////////////                                                    
                                                    
                                                    
       let detectionNet, recognitionNet, landmarkNet                                      
       let minConfidence = 0.2    
       let maxDistance  = 0.4                                             
                                                    
       const fullFaceDescriptions = (await faceapi.allFaces(myImg, minConfidence)).map(fd => fd.forSize(width, height))
       
       // possible repeat canvas here                                             
                                                    
       const canvas = document.getElementById('overlay')
       canvas.width = myImg.width
       canvas.height = myImg.height                                             
                                                    
        fullFaceDescriptions.forEach(({ detection, descriptor }) => {
        faceapi.drawDetection(canvas, [detection], { withScore: false })
                           
        console.log('fullFaceDescriptions')  
        console.log(fullFaceDescriptions)                                         
                                                    
                                                    
        //console.log(descriptor)
                                                    
                                                
        //console.log('descriptor')                    
       // console.log(descriptor)              
        
       // console.log('trainDescriptorsByClass')        
       // console.log(document.trainDescriptorsByClass) 
                                                    
                                                    
        const bestMatch = getBestMatch(document.trainDescriptorsByClass, descriptor)
              
                                                    
                                                    
                                                    
        const text = `${bestMatch.distance < maxDistance ? bestMatch.className : 'unkown'} (${bestMatch.distance})`
                                                    
                   
      //  console.log(text)                                                      
                                                    
        const { x, y, height: boxHeight } = detection[2].getBox()    // need to fix this
                                                    
     //   const x = 200
      //  const y = 100
      //   const boxHeight = 60
          //  console.log('document.detect2')  
          //  console.log(document.detect2)   
                                                    
        faceapi.drawText(
          canvas.getContext('2d'),
          x,
          y + boxHeight,
          text,
          Object.assign(faceapi.getDefaultDrawOptions(), { color: 'red', fontSize: 16 })
        )
      })
                                                       
                                                    
                                                    
                                                    
                                                    
                                                    
                                                    
                                                    
                                                    
                                                    
                                                    
                                                    
                                                    
                                                    
                                                    
                                                    
                                                    
                                                    
 ////////////////////////////////////////////                                                   
                                                    
// free memory for face image tensors after we computed their descriptors
alignedFaceTensors.forEach(t => t.dispose())

await console.log('each tensor disposed')   
await myImageToCanvas()                                                    
                                                    
    })()
}">






<input type=hidden id="myLoadDescriptors" value="Test making descriptors" onclick="{
( async function (){
   trainDescriptorsByClass = await initTrainDescriptorsByClass(faceapi.recognitionNet, 1)
                                                                                   
    console.log('trainDescriptorsByClass')  
    await console.log(document.trainDescriptorsByClass)
    })()
}"><br><br>


                
<img id="myImg01"    src="bigbang01.png" crossorigin="" ondrop="{ 
     myLoadImage(event)
  }" ondragover="{
    event.preventDefault(); 
    event.stopPropagation();
    event.dataTransfer.dropEffect = 'copy';
    //document.getElementById('myDiv02').innerHTML = 'drag over'
    //document.getElementById('myDetectButton').click()   // activate the detect function
}"   /><br>


<canvas id="myCanvas01"  > </canvas><br>


<canvas id="overlay" /><br>







