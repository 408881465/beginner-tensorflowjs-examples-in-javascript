

<!-- from https://github.com/tensorflow/tfjs-models/tree/master/posenet/demos  -->





<!DOCTYPE html>
<html>

<!-- Important script tags used by this program. Always best if they have a version number  -->   
    
    <script src="https://cdn.jsdelivr.net/npm/dat.gui@0.7.5/build/dat.gui.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.0.0"> </script> 
    <script src="https://unpkg.com/@tensorflow-models/posenet"></script> 
  <!--  <script src="stats.min.js"></script> -->  <!--  Got rid of this   -->
    
 
<!-- Following script files have just been included in this file  -->      
  <!--  <script src="demo_util.js"></script>   --> 
  <!--  <script src="camera.js"></script>    -->

 <script>
     
 // global variables
   let myPoseNetVersion = 0.75   // 0.50, 0.75, 1.00, or 1.01  
   let myAlgorithm = 'single-pose'
   let myFacingMode = 'environment'
   let myMultiplier = 0.75      
   let myImageScaleFactor = 0.5
   let myFlipHorizontal = false 
   let myOutputStride = 16  
   let myMaxPoseDetections = 5  
   let myScoreThreshold = 0.8 
   let myNmsRadius = 10 
   
   
   let myMinPoseConfidence = 1
   let myMinPartConfidence = 1
   let myShowVideo = true
   let myShowSkeleton = true
   let myShowBoundingBox = true    
     
</script>    
    
    
        
<!-- **************************************************************************************************  -->
<!-- **************************************** start of Orignal demo_util.js *****************************  -->
<!-- **************************************************************************************************  -->  
    
    
   <script> 
    
    
    
    
//import * as posenet from '@tensorflow-models/posenet';
//import * as tf from '@tensorflow/tfjs';

const color = 'aqua';
const boundingBoxColor = 'red';
const lineWidth = 2;

function toTuple({y, x}) {
  return [y, x];
}

///export function drawPoint(ctx, y, x, r, color) {
function drawPoint(ctx, y, x, r, color) {
  ctx.beginPath();
  ctx.arc(x, y, r, 0, 2 * Math.PI);
  ctx.fillStyle = color;
  ctx.fill();
}

/**
 * Draws a line on a canvas, i.e. a joint
 */
///export function drawSegment([ay, ax], [by, bx], color, scale, ctx) {
function drawSegment([ay, ax], [by, bx], color, scale, ctx) {
  ctx.beginPath();
  ctx.moveTo(ax * scale, ay * scale);
  ctx.lineTo(bx * scale, by * scale);
  ctx.lineWidth = lineWidth;
  ctx.strokeStyle = color;
  ctx.stroke();
}

/**
 * Draws a pose skeleton by looking up all adjacent keypoints/joints
 */
///export function drawSkeleton(keypoints, minConfidence, ctx, scale = 1) {
function drawSkeleton(keypoints, minConfidence, ctx, scale = 1) {
  const adjacentKeyPoints =
      posenet.getAdjacentKeyPoints(keypoints, minConfidence);

  adjacentKeyPoints.forEach((keypoints) => {
    drawSegment(
        toTuple(keypoints[0].position), toTuple(keypoints[1].position), color,
        scale, ctx);
  });
}

/**
 * Draw pose keypoints onto a canvas
 */
///export function drawKeypoints(keypoints, minConfidence, ctx, scale = 1) {
function drawKeypoints(keypoints, minConfidence, ctx, scale = 1) {
  for (let i = 0; i < keypoints.length; i++) {
    const keypoint = keypoints[i];

    if (keypoint.score < minConfidence) {
      continue;
    }

    const {y, x} = keypoint.position;
    drawPoint(ctx, y * scale, x * scale, 3, color);
  }
}

/**
 * Draw the bounding box of a pose. For example, for a whole person standing
 * in an image, the bounding box will begin at the nose and extend to one of
 * ankles
 */
///export function drawBoundingBox(keypoints, ctx) {
function drawBoundingBox(keypoints, ctx) {
  const boundingBox = posenet.getBoundingBox(keypoints);

  ctx.rect(
      boundingBox.minX, boundingBox.minY, boundingBox.maxX - boundingBox.minX,
      boundingBox.maxY - boundingBox.minY);

  ctx.strokeStyle = boundingBoxColor;
  ctx.stroke();
}

/**
 * Converts an arary of pixel data into an ImageData object
 */
///export async function renderToCanvas(a, ctx) {
async function renderToCanvas(a, ctx) {
  const [height, width] = a.shape;
  const imageData = new ImageData(width, height);

  const data = await a.data();

  for (let i = 0; i < height * width; ++i) {
    const j = i * 4;
    const k = i * 3;

    imageData.data[j + 0] = data[k + 0];
    imageData.data[j + 1] = data[k + 1];
    imageData.data[j + 2] = data[k + 2];
    imageData.data[j + 3] = 255;
  }

  ctx.putImageData(imageData, 0, 0);
}

/**
 * Draw an image on a canvas
 */
///export function renderImageToCanvas(image, size, canvas) {
function renderImageToCanvas(image, size, canvas) {
  canvas.width = size[0];
  canvas.height = size[1];
  const ctx = canvas.getContext('2d');

  ctx.drawImage(image, 0, 0);
}

/**
 * Draw heatmap values, one of the model outputs, on to the canvas
 * Read our blog post for a description of PoseNet's heatmap outputs
 * https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5
 */
///export function drawHeatMapValues(heatMapValues, outputStride, canvas) {
function drawHeatMapValues(heatMapValues, outputStride, canvas) {
  const ctx = canvas.getContext('2d');
  const radius = 5;
  const scaledValues = heatMapValues.mul(tf.scalar(outputStride, 'int32'));

  drawPoints(ctx, scaledValues, radius, color);
}

/**
 * Used by the drawHeatMapValues method to draw heatmap points on to
 * the canvas
 */
function drawPoints(ctx, points, radius, color) {
  const data = points.buffer().values;

  for (let i = 0; i < data.length; i += 2) {
    const pointY = data[i];
    const pointX = data[i + 1];

    if (pointX !== 0 && pointY !== 0) {
      ctx.beginPath();
      ctx.arc(pointX, pointY, radius, 0, 2 * Math.PI);
      ctx.fillStyle = color;
      ctx.fill();
    }
  }
}

/**
 * Draw offset vector values, one of the model outputs, on to the canvas
 * Read our blog post for a description of PoseNet's offset vector outputs
 * https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5
 */
///export function drawOffsetVectors(
function drawOffsetVectors(
    heatMapValues, offsets, outputStride, scale = 1, ctx) {
  const offsetPoints =
      posenet.singlePose.getOffsetPoints(heatMapValues, outputStride, offsets);

  const heatmapData = heatMapValues.buffer().values;
  const offsetPointsData = offsetPoints.buffer().values;

  for (let i = 0; i < heatmapData.length; i += 2) {
    const heatmapY = heatmapData[i] * outputStride;
    const heatmapX = heatmapData[i + 1] * outputStride;
    const offsetPointY = offsetPointsData[i];
    const offsetPointX = offsetPointsData[i + 1];

    drawSegment(
        [heatmapY, heatmapX], [offsetPointY, offsetPointX], color, scale, ctx);
  }
}    
    
    
    
    
    
    
    </script>   
    
    
    
<!-- **************************************************************************************************  -->
<!-- **************************************** end of Orignal demo_util.js *****************************  -->
<!-- **************************************************************************************************  -->
    

    

    
    
    
    
    
    
    
 
<!-- **************************************************************************************************  -->
<!-- **************************************** start of Orignal Camera.js ******************************  -->
<!-- **************************************************************************************************  -->   
    
    
 <script>   
    
    
//import * as posenet from '@tensorflow-models/posenet';
//import dat from 'dat.gui';
//import Stats from 'stats.js';

//import {drawBoundingBox, drawKeypoints, drawSkeleton} from './demo_util';

const videoWidth = 600;
const videoHeight = 500;
///const stats = new Stats();

function isAndroid() {
  return /Android/i.test(navigator.userAgent);
}

function isiOS() {
  return /iPhone|iPad|iPod/i.test(navigator.userAgent);
}

function isMobile() {
  return isAndroid() || isiOS();
}

/**
 * Loads a the camera to be used in the demo
 *
 */
async function setupCamera() {
  if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
    throw new Error(
        'Browser API navigator.mediaDevices.getUserMedia not available');
  }

  const video = document.getElementById('video');
  video.width = videoWidth;
  video.height = videoHeight;

  const mobile = isMobile();
  const stream = await navigator.mediaDevices.getUserMedia({
    'audio': false,
    'video': {
      facingMode: myFacingMode,
      width: mobile ? undefined : videoWidth,
      height: mobile ? undefined : videoHeight,
    },
  });
  video.srcObject = stream;

  return new Promise((resolve) => {
    video.onloadedmetadata = () => {
      resolve(video);
    };
  });
}

async function loadVideo() {
  const video = await setupCamera();
  video.play();

  return video;
}

const guiState = {
  algorithm: 'multi-pose',
  input: {
    mobileNetArchitecture: isMobile() ? '0.50' : '0.75',
    outputStride: 16,
    imageScaleFactor: 0.5,
  },
  singlePoseDetection: {
    minPoseConfidence: 0.1,
    minPartConfidence: 0.5,
  },
  multiPoseDetection: {
    maxPoseDetections: 5,
    minPoseConfidence: 0.15,
    minPartConfidence: 0.1,
    nmsRadius: 30.0,
  },
  output: {
    showVideo: true,
    showSkeleton: true,
    showPoints: true,
    showBoundingBox: false,
  },
  net: null,
};

     
     
     
     
     
     
     
     
     
     
     
     
/**
 * Sets up dat.gui controller on the top-right of the window
 */
function setupGui(cameras, net) {
  guiState.net = net;

  if (cameras.length > 0) {
    guiState.camera = cameras[0].deviceId;
  }

  const gui = new dat.GUI({width: 300});

  // The single-pose algorithm is faster and simpler but requires only one
  // person to be in the frame or results will be innaccurate. Multi-pose works
  // for more than 1 person
  const algorithmController =
      gui.add(guiState, 'algorithm', ['single-pose', 'multi-pose']);

  // The input parameters have the most effect on accuracy and speed of the
  // network
  let input = gui.addFolder('Input');
  // Architecture: there are a few PoseNet models varying in size and
  // accuracy. 1.01 is the largest, but will be the slowest. 0.50 is the
  // fastest, but least accurate.
  const architectureController = input.add(
      guiState.input, 'mobileNetArchitecture',
      ['1.01', '1.00', '0.75', '0.50']);
  // Output stride:  Internally, this parameter affects the height and width of
  // the layers in the neural network. The lower the value of the output stride
  // the higher the accuracy but slower the speed, the higher the value the
  // faster the speed but lower the accuracy.
  input.add(guiState.input, 'outputStride', [8, 16, 32]);
  // Image scale factor: What to scale the image by before feeding it through
  // the network.
  input.add(guiState.input, 'imageScaleFactor').min(0.2).max(1.0);
  input.open();

  // Pose confidence: the overall confidence in the estimation of a person's
  // pose (i.e. a person detected in a frame)
  // Min part confidence: the confidence that a particular estimated keypoint
  // position is accurate (i.e. the elbow's position)
  let single = gui.addFolder('Single Pose Detection');
  single.add(guiState.singlePoseDetection, 'minPoseConfidence', 0.0, 1.0);
  single.add(guiState.singlePoseDetection, 'minPartConfidence', 0.0, 1.0);

  let multi = gui.addFolder('Multi Pose Detection');
  multi.add(guiState.multiPoseDetection, 'maxPoseDetections')
      .min(1)
      .max(20)
      .step(1);
  multi.add(guiState.multiPoseDetection, 'minPoseConfidence', 0.0, 1.0);
  multi.add(guiState.multiPoseDetection, 'minPartConfidence', 0.0, 1.0);
  // nms Radius: controls the minimum distance between poses that are returned
  // defaults to 20, which is probably fine for most use cases
  multi.add(guiState.multiPoseDetection, 'nmsRadius').min(0.0).max(40.0);
  multi.open();

  let output = gui.addFolder('Output');
  output.add(guiState.output, 'showVideo');
  output.add(guiState.output, 'showSkeleton');
  output.add(guiState.output, 'showPoints');
  output.add(guiState.output, 'showBoundingBox');
  output.open();


  architectureController.onChange(function(architecture) {
    guiState.changeToArchitecture = architecture;
  });

  algorithmController.onChange(function(value) {
    switch (guiState.algorithm) {
      case 'single-pose':
        multi.close();
        single.open();
        break;
      case 'multi-pose':
        single.close();
        multi.open();
        break;
    }
  });
}

/**
 * Sets up a frames per second panel on the top-left of the window
 */
///function setupFPS() {
///  stats.showPanel(0);  // 0: fps, 1: ms, 2: mb, 3+: custom
///  document.body.appendChild(stats.dom);
///}

     
     
     
     
     
     
     
/**
 * Feeds an image to posenet to estimate poses - this is where the magic
 * happens. This function loops with a requestAnimationFrame method.
 */
function detectPoseInRealTime(video, net) {
  const canvas = document.getElementById('output');
  const ctx = canvas.getContext('2d');
  // since images are being fed from a webcam
 /// const flipHorizontal = true;

  canvas.width = videoWidth;
  canvas.height = videoHeight;

  async function poseDetectionFrame() {
   // if (guiState.changeToArchitecture) {
      // Important to purge variables and free up GPU memory
     // guiState.net.dispose();

      // Load the PoseNet model weights for either the 0.50, 0.75, 1.00, or 1.01
      // version
   //   guiState.net = await posenet.load(+guiState.changeToArchitecture);

     // guiState.changeToArchitecture = null;
  //  }

    // Begin monitoring code for frames per second
    ///stats.begin();

    // Scale an image down to a certain factor. Too large of an image will slow
    // down the GPU
    const imageScaleFactor = myImageScaleFactor;
    const outputStride = myOutputStride;

    let poses = [];
    let minPoseConfidence;
    let minPartConfidence;
    switch (myAlgorithm) {
      case 'single-pose':
        const pose = await estimateSinglePose(
            video, myImageScaleFactor, myFlipHorizontal, myOutputStride);
        poses.push(pose);

        minPoseConfidence = myMinPoseConfidence;
        minPartConfidence = myMinPartConfidence;
        break;
      case 'multi-pose':
        poses = await estimateMultiplePoses(
            video, myImageScaleFactor, myFlipHorizontal, myOutputStride,
            myMaxPoseDetections,
            myMinPartConfidence,
            myNmsRadius);

        minPoseConfidence = myMinPoseConfidence;
        minPartConfidence = myMinPartConfidence;
        break;
    }

    ctx.clearRect(0, 0, videoWidth, videoHeight);

    if (myShowVideo) {
      ctx.save();
      ctx.scale(-1, 1);
      ctx.translate(-videoWidth, 0);
      ctx.drawImage(video, 0, 0, videoWidth, videoHeight);
      ctx.restore();
    }

    // For each pose (i.e. person) detected in an image, loop through the poses
    // and draw the resulting skeleton and keypoints if over certain confidence
    // scores
    poses.forEach(({score, keypoints}) => {
      if (score >= minPoseConfidence) {
        if (myShowPoints) {
          drawKeypoints(keypoints, minPartConfidence, ctx);
        }
        if (myShowSkeleton) {
          drawSkeleton(keypoints, minPartConfidence, ctx);
        }
        if (myShowBoundingBox) {
          drawBoundingBox(keypoints, ctx);
        }
      }
    });

    // End monitoring code for frames per second
   /// stats.end();

    requestAnimationFrame(poseDetectionFrame);
  }

  poseDetectionFrame();
}

/**
 * Kicks off the demo by loading the posenet model, finding and loading
 * available camera devices, and setting off the detectPoseInRealTime function.
 */
///export async function bindPage() {
async function bindPage() {
  // Load the PoseNet model weights with architecture 0.75
  document.getElementById('info').innerHTML = 'loading....'  
  net = await posenet.load(myPoseNetVersion);   // make it global   0.50, 0.75, 1.00, or 1.01

  document.getElementById('info').innerHTML = ''  
  ///document.getElementById('loading').style.display = 'none';
 /// document.getElementById('main').style.display = 'block';   

  let video;

  try {
    video = await loadVideo(
  } catch (e) {
    let info = document.getElementById('info');
    info.textContent = 'this browser does not support video capture,' +
        'or this device does not have a camera';
    //info.style.display = 'block';
    throw e;
  }

 /// setupGui([], net);
  ///setupFPS();
  detectPoseInRealTime(video, net);
}

navigator.getUserMedia = navigator.getUserMedia ||
    navigator.webkitGetUserMedia || navigator.mozGetUserMedia;
// kick off the demo
///bindPage();  
    
    
 </script>   

 
<!-- **************************************************************************************************  -->
<!-- **************************************** end of Orignal Camera.js ******************************  -->
<!-- **************************************************************************************************  -->   
    
    
    
  
    
    
    
    
<!-- **************************************************************************************************  -->
<!-- **************************************** start of Orignal HTML ***********************************  -->
<!-- **************************************************************************************************  -->




<head>
    <title>PoseNet - Camera Feed Demo</title>

    <meta name="viewport" content="width=device-width, initial-scale=1">
</head>

<body>


    <div id="info" >
    </div>
    

    <div id='main' >
        <video id="video" playsinline style=" -moz-transform: scaleX(-1);
        -o-transform: scaleX(-1);
        -webkit-transform: scaleX(-1);
        transform: scaleX(-1);
        display: none;
        ">
        </video>
        <canvas id="output" />
    </div>

<div>
             
<h2 align=center>Posenet by Dan Oved, converted to a single Webpage by Jeremy Ellis</h2>
    I have tried to simplify Posenet with this single file html/javascript <a href="https://github.com/hpssjellis/beginner-tensorflowjs-examples-in-javascript/blob/master/tfjsv1/tfjs01posenetcam.html">Github here</a> <br>
Posenet by Dan Oved using Tensorflowjs Machine Learning in Javascript for Node and your browser. Orignial github at
<a href="https://github.com/tensorflow/tfjs-models/tree/master/posenet"> Posenet Github </a>
Original Demo at <a href="https://storage.googleapis.com/tfjs-models/demos/posenet/camera.html"> Posenet Demo Here</a><br><br>
    
 Converted by Jeremy Ellis Twitter <a href="https://twitter.com/rocksetta?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor">@rocksetta</a> Machine Learning Website at <a href="https://www.rocksetta.com/tensorflowjs/">https://www.rocksetta.com/tensorflowjs/</a>    
<br>Use at your own risk!<br>    <br> <br> <br> 
 
    Working on the following <br><br>
        
 Set variables<br>   
    
    
     
<input type=button value="Defults" onclick="{ 
   myPoseNetVersion = 0.75   // 0.50, 0.75, 1.00, or 1.01  
   myAlgorithm = 'single-pose'
   myMultiplier = 1.01      
   myImageScaleFactor = 1.0
   myFlipHorizontal = true
   myOutputStride = 32  
   myMaxPoseDetections = 10  
   myScoreThreshold = 0.5 
   myNmsRadius = 20  
}">    
    
<input type=button value="defaults changed" onclick="{  
   myPoseNetVersion = 1.01   // 0.50, 0.75, 1.00, or 1.01 
   myAlgorithm = 'multi-pose'                                                  
   myMultiplier = 0.75      
   myImageScaleFactor = 0.5
   myFlipHorizontal = false 
   myOutputStride = 16  
   myMaxPoseDetections = 5  
   myScoreThreshold = 0.8 
   myNmsRadius = 10  
}">     <br>   
    
    
<input type=button value="user/front" onclick="{                                                                                                                                                                                                                
   myFacingMode = 'user'  
}">           
 
<input type=button value="environment/back Camera" onclick="{
   myFacingMode = 'environment'                                                                                                                                                                                                                      
}"> <br><br>
   
    
    
    
    
    
    
Now click start    
     
<input type=button value="Start" onclick="{                                                                                                                                                                                                                
   bindPage() 
}">   
    Refresh page to switch cameras<br><br>           
     
    
<input type=button value="Switch Pose Mode" onclick="{
  alert()                                                
}">      
    
    
<input type=button value="Switch Pose Mode" onclick="{
  alert()                                                
}">           
      
            
<input type=button value="Set Output Stride to" onclick="{
  alert()                                                
}">        
            
<input type=button value="Set Image Scale Factor to" onclick="{
  alert()                                                
}">  
    

    
    
<ul>Defaults:
   <li> pose the object containing all the output data
   <li> multiplier = 1.01      Smaller value faster less accurate, larger value slower but better
  <li>image     can be any of  ImageData|HTMLImageElement|HTMLCanvasElement|HTMLVideoElement
  <li>imageScaleFactor = 0.5  0.2 to 1.0
  <li>flipHorizontal = false
  <li>outputStride = 16    32, 16, 8
  <ul> following only for Multi-Person Pose Estimation    
      
  <li>maxPoseDetections = 5
  <li>scoreThreshold = 0.5
  <li>nmsRadius = 20   must be >= 0
  <li>
  </ul>    
  <li>
  <li>      
   
</ul>
</div>    
    
</body>

</html>


<!-- **************************************************************************************************  -->
<!-- ************************************* End of Original HTML ***************************************  -->
<!-- **************************************************************************************************  -->
